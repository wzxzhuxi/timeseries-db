use std::collections::BTreeMap;
use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex, RwLock};
use std::time::{SystemTime, UNIX_EPOCH};
use std::io::Result;

use super::{DataPoint, Memtable, SSTable, GorillaCompressor, SeriesData};

#[derive(Debug)]
pub struct TimeSeriesDB {
    memtable: Arc<RwLock<Memtable>>,
    sstables: Arc<Mutex<Vec<SSTable>>>,
    data_dir: PathBuf,
    memtable_threshold: usize,
}

impl TimeSeriesDB {
    pub fn new<P: AsRef<Path>>(data_dir: P, memtable_threshold: usize) -> Result<Self> {
        let data_dir = data_dir.as_ref().to_path_buf();
        std::fs::create_dir_all(&data_dir)?;

        let mut sstables = Vec::new();
        if let Ok(entries) = std::fs::read_dir(&data_dir) {
            // for entry in entries {
            //     if let Ok(entry) = entry {
            //         let path = entry.path();
            //         if path.extension().and_then(|s| s.to_str()) == Some("data") {
            //             sstables.push(SSTable::new(path)?);
            //         }
            //     }
            // }
            for entry in entries.flatten() {
    let path = entry.path();
    if path.extension().and_then(|s| s.to_str()) == Some("data") {
        sstables.push(SSTable::new(path)?);
    }
}
        }

        Ok(Self {
            memtable: Arc::new(RwLock::new(Memtable::new(memtable_threshold))),
            sstables: Arc::new(Mutex::new(sstables)),
            data_dir,
            memtable_threshold,
        })
    }

    pub async fn insert(&self, series_key: String, datapoint: DataPoint) -> Result<()> {
        // æ£€æŸ¥æ˜¯å¦éœ€è¦flushï¼Œåœ¨é”å¤–è¿›è¡Œ
        let should_flush = {
            let mut memtable = self.memtable.write().unwrap();
            memtable.insert(series_key, datapoint);
            memtable.is_full()
        };

        if should_flush {
            self.flush_memtable().await?;
        }

        Ok(()) // ä¿®å¤ï¼šæ·»åŠ  () å‚æ•°
    }

    pub async fn update(&self, series_key: &str, timestamp: u64, new_value: f64) -> Result<bool> {
        // é¦–å…ˆå°è¯•åœ¨å†…å­˜è¡¨ä¸­æ›´æ–°
        let updated_in_memtable = {
            let mut memtable = self.memtable.write().unwrap();
            memtable.update(series_key, timestamp, new_value)
        };

        if updated_in_memtable {
            return Ok(true);
        }

        // åœ¨SSTableä¸­æŸ¥æ‰¾å¹¶æ›´æ–°ï¼Œé¿å…è·¨awaitæŒæœ‰é”
        let mut sstables = self.sstables.lock().unwrap();
        for sstable in sstables.iter_mut() {
            if sstable.update_datapoint(series_key, timestamp, new_value)? {
                return Ok(true);
            }
        }
        drop(sstables);

        Ok(false)
    }

    pub async fn delete(&self, series_key: &str, timestamp: Option<u64>) -> Result<bool> {
        // é¦–å…ˆå°è¯•åœ¨å†…å­˜è¡¨ä¸­åˆ é™¤
        let deleted_from_memtable = {
            let mut memtable = self.memtable.write().unwrap();
            memtable.delete(series_key, timestamp)
        };

        // åœ¨SSTableä¸­åˆ é™¤
        let mut deleted_from_sstable = false;
        {
            let mut sstables = self.sstables.lock().unwrap();
            
            for sstable in sstables.iter_mut() {
                if sstable.delete_datapoint(series_key, timestamp)? {
                    deleted_from_sstable = true;
                }
            }

            // æ¸…ç†ç©ºçš„SSTableæ–‡ä»¶
            let mut indices_to_remove = Vec::new();
            for (index, sstable) in sstables.iter_mut().enumerate() {
                match sstable.get_all_series_keys() {
                    Ok(keys) => {
                        if keys.is_empty() {
                            indices_to_remove.push(index);
                        }
                    }
                    Err(_) => {
                        // ä¿ç•™æ— æ³•è¯»å–çš„æ–‡ä»¶
                    }
                }
            }
            
            // ä»åå¾€å‰åˆ é™¤ï¼Œé¿å…ç´¢å¼•é”™ä¹±
            for &index in indices_to_remove.iter().rev() {
                sstables.remove(index);
            }
        }

        Ok(deleted_from_memtable || deleted_from_sstable)
    }

    pub async fn query_range(&self, series_key: &str, start_time: Option<u64>, end_time: Option<u64>) -> Result<Vec<DataPoint>> {
    let mut results = Vec::new();

    // æŸ¥è¯¢å†…å­˜è¡¨
    {
        let memtable = self.memtable.read().unwrap();
        let memtable_results = memtable.query(series_key, start_time, end_time);
        println!("ğŸ” å†…å­˜è¡¨æŸ¥è¯¢: {} ä¸ªæ•°æ®ç‚¹", memtable_results.len());
        results.extend(memtable_results);
    }

    // æŸ¥è¯¢SSTableæ–‡ä»¶
    {
        let mut sstables = self.sstables.lock().unwrap();
        println!("ğŸ—„ï¸ æ£€æŸ¥ {} ä¸ªSSTableæ–‡ä»¶", sstables.len());
        
        for (i, sstable) in sstables.iter_mut().enumerate() {
            match sstable.query_series(series_key, start_time, end_time) {
                Ok(sstable_results) => {
                    println!("  SSTable {}: {} ä¸ªæ•°æ®ç‚¹", i, sstable_results.len());
                    results.extend(sstable_results);
                }
                Err(e) => {
                    println!("  SSTable {} æŸ¥è¯¢å¤±è´¥: {}", i, e);
                    continue;
                }
            }
        }
    }

    // æŒ‰æ—¶é—´æˆ³æ’åºå¹¶å»é‡
    results.sort_by_key(|dp| dp.timestamp);
    let before_dedup = results.len();
    results.dedup_by_key(|dp| dp.timestamp);
    
    if before_dedup != results.len() {
        println!("ğŸ”„ å»é‡: {} -> {} ä¸ªæ•°æ®ç‚¹", before_dedup, results.len());
    }
    
    println!("ğŸ“Š æœ€ç»ˆæŸ¥è¯¢ç»“æœ: {} ä¸ªæ•°æ®ç‚¹", results.len());
    Ok(results)
    }


    pub async fn get_all_series(&self) -> Result<Vec<String>> {
        let mut series_keys = std::collections::HashSet::new();

        // è·å–å†…å­˜è¡¨ä¸­çš„ç³»åˆ—
        {
            let memtable = self.memtable.read().unwrap();
            for key in memtable.get_data().keys() {
                series_keys.insert(key.clone());
            }
        }

        // è·å–SSTableä¸­çš„ç³»åˆ—
        {
            let mut sstables = self.sstables.lock().unwrap();
            for sstable in sstables.iter_mut() {
                match sstable.get_all_series_keys() {
                    Ok(keys) => {
                        for key in keys {
                            series_keys.insert(key);
                        }
                    }
                    Err(e) => {
                        tracing::warn!("è¯»å–SSTableç³»åˆ—é”®å¤±è´¥: {}", e);
                    }
                }
            }
        }

        Ok(series_keys.into_iter().collect())
    }

    async fn flush_memtable(&self) -> Result<()> {
        // è·å–æ•°æ®å¹¶æ¸…ç©ºå†…å­˜è¡¨ï¼Œç¡®ä¿é”ä¸è·¨è¶Šawait
        let data = {
            let mut memtable = self.memtable.write().unwrap();
            let data = memtable.get_data().clone();
            memtable.clear();
            data
        };

        if data.is_empty() {
            return Ok(()); // ä¿®å¤ï¼šè¿™é‡Œå°±æ˜¯ç¬¬196è¡Œï¼Œéœ€è¦æ·»åŠ  () å‚æ•°
        }

        let timestamp = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_secs();
        
        let sstable_path = self.data_dir.join(format!("sstable_{}.data", timestamp));
        let mut sstable = SSTable::new(sstable_path)?;

        let mut series_data_list = Vec::new();
        
        for (series_key, datapoints) in data {
            if datapoints.is_empty() {
                continue;
            }

            let mut compressor = GorillaCompressor::new();
            let mut min_timestamp = u64::MAX;
            let mut max_timestamp = 0u64;
            let mut tags = BTreeMap::new();

            for datapoint in &datapoints {
                compressor.compress_datapoint(datapoint.timestamp, datapoint.value);
                min_timestamp = min_timestamp.min(datapoint.timestamp);
                max_timestamp = max_timestamp.max(datapoint.timestamp);
                
                if tags.is_empty() {
                    tags = datapoint.tags.clone();
                }
            }
            
            let compressed_data = compressor.finish();
            
            let series_data = SeriesData {
                series_key,
                compressed_data,
                tags,
                min_timestamp,
                max_timestamp,
                count: datapoints.len(),
            };

            series_data_list.push(series_data);
        }

        sstable.write_data(&series_data_list)?;
        
        // æ·»åŠ æ–°çš„SSTableï¼Œé”çš„ä½œç”¨åŸŸå¾ˆå°
        {
            let mut sstables = self.sstables.lock().unwrap();
            sstables.push(sstable);
        }

        tracing::info!("å†…å­˜è¡¨å·²åˆ·æ–°åˆ°SSTableï¼ŒåŒ…å« {} ä¸ªç³»åˆ—", series_data_list.len());

        Ok(()) // ä¿®å¤ï¼šæ·»åŠ  () å‚æ•°
    }

    pub async fn compact(&self) -> Result<()> {
        tracing::info!("å¼€å§‹æ‰§è¡Œcompactionæ“ä½œ");
        
        // è·å–æ‰€æœ‰SSTableæ•°æ®ï¼Œé¿å…é•¿æ—¶é—´æŒæœ‰é”
        let all_series_data = {
            let mut sstables = self.sstables.lock().unwrap();
            
            if sstables.len() < 2 {
                tracing::info!("SSTableæ•°é‡ä¸è¶³ï¼Œè·³è¿‡compaction");
                return Ok(()); // ä¿®å¤ï¼šæ·»åŠ  () å‚æ•°
            }

            let mut all_series_data = BTreeMap::new();

            // è¯»å–æ‰€æœ‰SSTableä¸­çš„æ•°æ®
            for sstable in sstables.iter_mut() {
                match sstable.get_all_series_keys() {
                    Ok(series_keys) => {
                        for series_key in series_keys {
                            match sstable.query_series(&series_key, None, None) {
                                Ok(datapoints) => {
                                    let entry = all_series_data.entry(series_key).or_insert_with(Vec::new);
                                    entry.extend(datapoints);
                                }
                                Err(e) => {
                                    tracing::warn!("è¯»å–ç³»åˆ—æ•°æ®å¤±è´¥: {}", e);
                                }
                            }
                        }
                    }
                    Err(e) => {
                        tracing::warn!("è¯»å–SSTableå¤±è´¥: {}", e);
                    }
                }
            }

            // åˆ é™¤æ—§çš„SSTableæ–‡ä»¶
            for sstable in sstables.iter() {
                if let Err(e) = sstable.delete_file() {
                    tracing::warn!("åˆ é™¤æ—§SSTableæ–‡ä»¶å¤±è´¥: {}", e);
                }
            }
            sstables.clear();

            all_series_data
        };

        // åˆ›å»ºæ–°çš„compacted SSTable
        if !all_series_data.is_empty() {
            let timestamp = SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_secs();
            
            let sstable_path = self.data_dir.join(format!("compacted_{}.data", timestamp));
            let mut new_sstable = SSTable::new(sstable_path)?;

            let mut series_data_list = Vec::new();

            for (series_key, mut datapoints) in all_series_data {
                // æŒ‰æ—¶é—´æˆ³æ’åºå¹¶å»é‡
                datapoints.sort_by_key(|dp| dp.timestamp);
                datapoints.dedup_by_key(|dp| dp.timestamp);

                if datapoints.is_empty() {
                    continue;
                }

                let mut compressor = GorillaCompressor::new();
                let mut min_timestamp = u64::MAX;
                let mut max_timestamp = 0u64;
                let mut tags = BTreeMap::new();

                for datapoint in &datapoints {
                    compressor.compress_datapoint(datapoint.timestamp, datapoint.value);
                    min_timestamp = min_timestamp.min(datapoint.timestamp);
                    max_timestamp = max_timestamp.max(datapoint.timestamp);
                    
                    if tags.is_empty() {
                        tags = datapoint.tags.clone();
                    }
                }

                let compressed_data = compressor.finish();
                
                let series_data = SeriesData {
                    series_key,
                    compressed_data,
                    tags,
                    min_timestamp,
                    max_timestamp,
                    count: datapoints.len(),
                };

                series_data_list.push(series_data);
            }

            new_sstable.write_data(&series_data_list)?;
            
            {
                let mut sstables = self.sstables.lock().unwrap();
                sstables.push(new_sstable);
            }

            tracing::info!("Compactionå®Œæˆï¼Œåˆå¹¶äº† {} ä¸ªç³»åˆ—", series_data_list.len());
        }

        Ok(()) // ä¿®å¤ï¼šæ·»åŠ  () å‚æ•°
    }

    pub async fn get_stats(&self) -> Result<DatabaseStats> {
        let memtable_size = {
            let memtable = self.memtable.read().unwrap();
            memtable.get_data().len()
        };

        let sstable_count = {
            let sstables = self.sstables.lock().unwrap();
            sstables.len()
        };

        let all_series = self.get_all_series().await?;
        let total_series = all_series.len();

        Ok(DatabaseStats {
            memtable_size,
            sstable_count,
            total_series,
        })
    }
}

#[derive(Debug, serde::Serialize)]
pub struct DatabaseStats {
    pub memtable_size: usize,
    pub sstable_count: usize,
    pub total_series: usize,
}

impl Clone for TimeSeriesDB {
    fn clone(&self) -> Self {
        Self {
            memtable: Arc::clone(&self.memtable),
            sstables: Arc::clone(&self.sstables),
            data_dir: self.data_dir.clone(),
            memtable_threshold: self.memtable_threshold,
        }
    }
}

